#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å››æ¨¡å‹å•ç‹¬æ¨ç†æµ‹è¯•
=================
1. TPU0: EfficientDet Lite2 448 ç›®æ ‡æ£€æµ‹
2. TPU0/1: DeepLabv3 m0.5 è¯­ä¹‰åˆ†å‰²
3. TPU1: MobileNet (NetVLADç‰¹å¾æå–)
4. CPU: NetVLAD å¤´éƒ¨ (WPCA512)

æµ‹é‡æ¯ä¸ªæ¨¡å‹çš„å†·å¯åŠ¨å’Œçƒ­è¿è¡Œè€—æ—¶
å›ºå®šCPUé¢‘ç‡ï¼Œä½¿ç”¨æ¨¡æ‹Ÿè¾“å…¥
"""

import json
import time
import numpy as np
import sys
import os
import torch
from typing import Optional

# pycoral åœ¨æŸäº›ç¯å¢ƒé‡Œä¸å¯ç”¨ï¼ˆç¼ºå°‘ pybindï¼‰ï¼›è¿™é‡Œåšå…¼å®¹å›é€€åˆ° tflite_runtime + edgetpu delegate
try:
    from pycoral.utils.edgetpu import make_interpreter as coral_make_interpreter  # type: ignore
except Exception:
    coral_make_interpreter = None

# æ¨¡å‹è·¯å¾„
# è¯´æ˜ï¼š
# - TPU æ¨¡å‹ï¼š*.tfliteï¼ˆedgetpu ç¼–è¯‘ç‰ˆï¼‰
# - CPU æ¨¡å‹ï¼šNetVLAD headï¼ˆPyTorch checkpointï¼‰
MODELS = {
    # Detectorï¼ˆå¯é€‰ï¼šSSD MobileNet / EfficientDetï¼‰
    "ssd_mobilenet_v2": "/home/10210/Desktop/ROS/models/ssd_mobilenet_v2_coco_edgetpu.tflite",
    # è”åˆç¼–è¯‘ç‰ˆï¼ˆä½ è¯´çš„ co-compile/jointï¼‰ï¼šä¸ mobilenet_v2 å…±äº«ç¼–è¯‘/ç¼“å­˜ï¼Œé€šå¸¸æ›´å¿«æ›´ç¨³
    "ssd_mobilenet_v2_joint": "/home/10210/Desktop/OS/real_time_analysis/ssd_mobilenet_v2_coco_quant_postprocess_joint_backbone_edgetpu.tflite",
    "efficientdet_lite2_448": "/home/10210/Desktop/OS/models_local/public/efficientdet_lite2_448_ptq_edgetpu.tflite",

    # Segmentation
    "deeplabv3_dm05": "/home/10210/Desktop/OS/models_local/public/deeplabv3_mnv2_dm05_pascal_quant_edgetpu.tflite",

    # NetVLAD feature extractor backbone (TPU)
    "mobilenet_v2": "/home/10210/Desktop/OS/models_local/public/mobilenet_v2_1.0_224_quant_edgetpu.tflite",
    "mobilenet_v2_joint": "/home/10210/Desktop/OS/real_time_analysis/mobilenet_v2_1.0_224_quant_joint_ssd_edgetpu.tflite",

    # NetVLAD head (CPU)
    "netvlad_head": "/home/10210/Desktop/ROS/models/mapillary_WPCA512.pth.tar",
}


def _create_interpreter(model_path: str, use_tpu: bool = True):
    """åˆ›å»ºè§£é‡Šå™¨å¹¶è¿”å›è¾“å…¥/è¾“å‡ºè¯¦æƒ…åŠæ¨¡æ‹Ÿè¾“å…¥ã€‚"""
    if use_tpu:
        if coral_make_interpreter is not None:
            # pycoral ä¼šè¯»å–ç¯å¢ƒå˜é‡ EDGETPU_DEVICEï¼ˆrun_burst_measurements ä¼šè®¾ç½®å®ƒï¼‰
            interpreter = coral_make_interpreter(model_path)
        else:
            # å›é€€ï¼štflite_runtime + EdgeTPU delegate
            from tflite_runtime.interpreter import Interpreter, load_delegate

            dev = os.environ.get("EDGETPU_DEVICE")
            delegate = load_delegate("libedgetpu.so.1", {"device": dev} if dev else {})
            interpreter = Interpreter(model_path=model_path, experimental_delegates=[delegate], num_threads=1)
    else:
        from tflite_runtime.interpreter import Interpreter
        interpreter = Interpreter(model_path=model_path, num_threads=1)
    interpreter.allocate_tensors()
    inp = interpreter.get_input_details()[0]
    out = interpreter.get_output_details()[0]
    dummy = _generate_dummy_input(inp)
    return interpreter, inp, out, dummy


def _generate_dummy_input(inp_detail):
    """æ ¹æ®è¾“å…¥å¼ é‡ä¿¡æ¯ç”Ÿæˆæ¨¡æ‹Ÿè¾“å…¥ã€‚"""
    input_shape = inp_detail['shape']
    input_dtype = inp_detail['dtype']
    if input_dtype == np.uint8:
        return np.random.randint(0, 256, input_shape, dtype=np.uint8)
    if input_dtype == np.int8:
        return np.random.randint(-128, 128, input_shape, dtype=np.int8)
    return np.random.random_sample(input_shape).astype(input_dtype)


def _run_warm_invocations(
    interpreter,
    inp_detail,
    out_detail,
    dummy,
    warm_repeats: int,
    warmup: int,
    sleep_between_ms: Optional[float] = None,
    idle_every: Optional[int] = None,
    idle_duration_ms: Optional[float] = None,
    capture_cycle: bool = False,
):
    """æ‰§è¡Œé¢„çƒ­ä¸æ­£å¼æ¨ç†å¾ªç¯ï¼Œå¯é€‰å‘¨æœŸä¸ç©ºé—²æ§åˆ¶ã€‚"""
    inp_index = inp_detail['index']
    out_index = out_detail['index']

    for _ in range(warmup):
        interpreter.set_tensor(inp_index, dummy)
        interpreter.invoke()
        _ = interpreter.get_tensor(out_index)

    warm_times = []
    cycle_times = [] if capture_cycle else None

    for i in range(warm_repeats):
        cycle_start = time.perf_counter()
        interpreter.set_tensor(inp_index, dummy)
        t0 = time.perf_counter()
        interpreter.invoke()
        t1 = time.perf_counter()
        _ = interpreter.get_tensor(out_index)
        warm_time_ms = (t1 - t0) * 1000.0
        warm_times.append(warm_time_ms)

        cycle_end = time.perf_counter()
        if capture_cycle:
            cycle_times.append((cycle_end - cycle_start) * 1000.0)

        if sleep_between_ms is not None:
            remain = (sleep_between_ms / 1000.0) - (cycle_end - cycle_start)
            if remain > 0:
                time.sleep(remain)

        if idle_every and idle_duration_ms and (i + 1) % idle_every == 0:
            time.sleep(idle_duration_ms / 1000.0)

    return warm_times, cycle_times


def measure_invoke_only(interpreter, input_tensor):
    """ä»…æµ‹é‡ invoke æ—¶é—´"""
    t0 = time.perf_counter()
    interpreter.invoke()
    t1 = time.perf_counter()
    return (t1 - t0) * 1000.0  # ms


def test_tpu_model(
    model_path: str,
    model_name: str,
    cold_repeats: int = 10,
    warm_repeats: int = 50,
    warmup: int = 50,
    use_tpu: bool = True,
    sleep_between_ms: Optional[float] = None,
    idle_every: Optional[int] = None,
    idle_duration_ms: Optional[float] = None,
    capture_cycle: bool = False,
):
    """æµ‹è¯• TPU/CPU æ¨¡å‹"""
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•æ¨¡å‹: {model_name}")
    print(f"è·¯å¾„: {model_path}")
    print(f"è®¾å¤‡: {'TPU' if use_tpu else 'CPU'}")
    print(f"{'='*60}")
    
    results = {
        'model_name': model_name,
        'model_path': model_path,
        'device': 'TPU' if use_tpu else 'CPU',
        'cold_start_ms': [],
        'warm_run_ms': [],
    }
    
    # 1. å†·å¯åŠ¨æµ‹è¯•ï¼šç¬¬ä¸€æ¬¡ invoke ä½œä¸ºå†·å¯åŠ¨
    print(f"\n[1/2] å†·å¯åŠ¨æµ‹è¯• (ä½¿ç”¨ç¬¬ä¸€æ¬¡ invoke)...")
    interpreter, inp, out, dummy = _create_interpreter(model_path, use_tpu=use_tpu)
    
    # å†·å¯åŠ¨ï¼šç¬¬ä¸€æ¬¡ invoke
    interpreter.set_tensor(inp['index'], dummy)
    t0 = time.perf_counter()
    interpreter.invoke()
    t1 = time.perf_counter()
    _ = interpreter.get_tensor(out['index'])
    cold_time = (t1 - t0) * 1000.0
    results['cold_start_ms'].append(cold_time)
    print(f"  å†·å¯åŠ¨: {cold_time:.2f} ms")
    
    # 2. çƒ­å¯åŠ¨æµ‹è¯•ï¼šç»§ç»­ç”¨åŒä¸€ä¸ªè§£é‡Šå™¨
    print(f"\n[2/2] çƒ­è¿è¡Œæµ‹è¯• (é¢„çƒ­ {warmup} æ¬¡ï¼Œæµ‹é‡ {warm_repeats} æ¬¡)...")
    
    warm_times, cycle_times = _run_warm_invocations(
        interpreter,
        inp,
        out,
        dummy,
        warm_repeats=warm_repeats,
        warmup=warmup,
        sleep_between_ms=sleep_between_ms,
        idle_every=idle_every,
        idle_duration_ms=idle_duration_ms,
        capture_cycle=capture_cycle,
    )

    for idx, warm_time in enumerate(warm_times, start=1):
        results['warm_run_ms'].append(warm_time)
        if idx % 10 == 0:
            print(f"  å®Œæˆ {idx}/{warm_repeats} æ¬¡")

    if cycle_times is not None:
        results['cycle_ms'] = cycle_times
    
    # ç»Ÿè®¡
    cold_avg = results['cold_start_ms'][0]  # åªæœ‰ä¸€æ¬¡å†·å¯åŠ¨
    warm_avg = np.mean(results['warm_run_ms'])
    warm_std = np.std(results['warm_run_ms'])
    
    print(f"\nğŸ“Š ç»Ÿè®¡ç»“æœ:")
    print(f"  å†·å¯åŠ¨: {cold_avg:.2f} ms (n=1)")
    print(f"  çƒ­è¿è¡Œ: {warm_avg:.2f} Â± {warm_std:.2f} ms (n={warm_repeats})")
    print(f"  åŠ é€Ÿæ¯”: {cold_avg/warm_avg:.2f}x")
    
    results['statistics'] = {
        'cold_avg_ms': cold_avg,
        'cold_std_ms': 0.0,
        'warm_avg_ms': warm_avg,
        'warm_std_ms': warm_std,
        'speedup': cold_avg / warm_avg if warm_avg > 0 else 0,
    }
    
    if cycle_times is not None:
        results.setdefault('statistics', {})['cycle_avg_ms'] = float(np.mean(cycle_times)) if cycle_times else 0.0
        results['statistics']['cycle_std_ms'] = float(np.std(cycle_times)) if cycle_times else 0.0

    results['metadata'] = {
        'sleep_between_ms': sleep_between_ms,
        'idle_every': idle_every,
        'idle_duration_ms': idle_duration_ms,
        'warmup': warmup,
        'warm_repeats': warm_repeats,
    }

    return results


def test_cpu_netvlad(
    model_path: str,
    model_name: str,
    cold_repeats: int = 10,
    warm_repeats: int = 50,
    warmup: int = 50,
    sleep_between_ms: Optional[float] = None,
    idle_every: Optional[int] = None,
    idle_duration_ms: Optional[float] = None,
    capture_cycle: bool = False,
):
    """æµ‹è¯• CPU NetVLAD å¤´éƒ¨"""
    print(f"\n{'='*60}")
    print(f"æµ‹è¯•æ¨¡å‹: {model_name}")
    print(f"è·¯å¾„: {model_path}")
    print(f"{'='*60}")
    
    results = {
        'model_name': model_name,
        'model_path': model_path,
        'cold_start_ms': [],
        'warm_run_ms': [],
    }
    
    # é¢„å…ˆç”Ÿæˆå›ºå®šçš„æƒé‡ï¼Œé¿å…æ¯æ¬¡ randn
    n_clusters = 64
    descriptor_dim = 1280
    conv_weights = torch.randn(n_clusters, descriptor_dim, 1, 1)
    cluster_centers = [torch.randn(descriptor_dim, 1) for _ in range(n_clusters)]
    
    # 1. å†·å¯åŠ¨æµ‹è¯•ï¼šåªæµ‹ç¬¬ä¸€æ¬¡ï¼ˆåŒ…å«åŠ è½½ checkpointï¼‰
    print(f"\n[1/2] å†·å¯åŠ¨æµ‹è¯• (ç¬¬ä¸€æ¬¡åŠ è½½)...")
    t0 = time.perf_counter()
    checkpoint = torch.load(model_path, map_location='cpu')
    
    # æ¨¡æ‹Ÿ NetVLAD è¾“å…¥
    input_features = torch.randn(1, 1280, 7, 7)
    
    # NetVLAD èšåˆ
    soft_assign = torch.nn.functional.conv2d(input_features, conv_weights)
    soft_assign = torch.nn.functional.softmax(soft_assign, dim=1)
    feature_flat = input_features.view(1, descriptor_dim, -1)
    soft_assign_flat = soft_assign.view(1, n_clusters, -1)
    
    residuals = []
    for k in range(n_clusters):
        residual = (feature_flat - cluster_centers[k]) * soft_assign_flat[:, k:k+1, :]
        residuals.append(residual.sum(dim=2))
    
    vlad = torch.cat(residuals, dim=1)
    vlad = torch.nn.functional.normalize(vlad, p=2, dim=1)
    
    # WPCA projection
    if 'WPCA' in checkpoint:
        wpca_matrix = checkpoint['WPCA']
        if isinstance(wpca_matrix, np.ndarray):
            wpca_matrix = torch.from_numpy(wpca_matrix).float()
        if wpca_matrix.shape[0] == vlad.shape[1]:
            final_descriptor = torch.matmul(vlad, wpca_matrix.t())
        else:
            final_descriptor = vlad[:, :512]
    else:
        final_descriptor = vlad[:, :512]
    
    final_descriptor = torch.nn.functional.normalize(final_descriptor, p=2, dim=1)
    
    t1 = time.perf_counter()
    cold_time = (t1 - t0) * 1000.0
    results['cold_start_ms'].append(cold_time)
    print(f"  å†·å¯åŠ¨ (å«åŠ è½½): {cold_time:.2f} ms")
    
    # 2. çƒ­å¯åŠ¨æµ‹è¯•ï¼šcheckpoint å·²åŠ è½½ï¼Œåªæµ‹æ¨ç†
    print(f"\n[2/2] çƒ­è¿è¡Œæµ‹è¯• (é¢„çƒ­ {warmup} æ¬¡ï¼Œæµ‹é‡ {warm_repeats} æ¬¡)...")
    
    def _netvlad_forward():
        soft_assign_local = torch.nn.functional.conv2d(input_features, conv_weights)
        soft_assign_local = torch.nn.functional.softmax(soft_assign_local, dim=1)
        feature_flat_local = input_features.view(1, descriptor_dim, -1)
        soft_assign_flat_local = soft_assign_local.view(1, n_clusters, -1)
        residuals_local = []
        for k_local in range(n_clusters):
            residual_local = (feature_flat_local - cluster_centers[k_local]) * soft_assign_flat_local[:, k_local:k_local+1, :]
            residuals_local.append(residual_local.sum(dim=2))
        vlad_local = torch.cat(residuals_local, dim=1)
        vlad_local = torch.nn.functional.normalize(vlad_local, p=2, dim=1)
        if 'WPCA' in checkpoint:
            wpca_matrix_local = checkpoint['WPCA']
            if isinstance(wpca_matrix_local, np.ndarray):
                wpca_matrix_local = torch.from_numpy(wpca_matrix_local).float()
            if wpca_matrix_local.shape[0] == vlad_local.shape[1]:
                final_descriptor_local = torch.matmul(vlad_local, wpca_matrix_local.t())
            else:
                final_descriptor_local = vlad_local[:, :512]
        else:
            final_descriptor_local = vlad_local[:, :512]
        return torch.nn.functional.normalize(final_descriptor_local, p=2, dim=1)

    # é¢„çƒ­ï¼ˆä¸è®¡æ—¶ï¼‰
    for _ in range(warmup):
        _netvlad_forward()

    # æµ‹é‡ï¼ˆè®¡æ—¶ä»…æ¨ç†éƒ¨åˆ†ï¼‰
    cycle_times = [] if capture_cycle else None
    for i in range(warm_repeats):
        cycle_start = time.perf_counter()
        t0 = time.perf_counter()
        _netvlad_forward()
        t1 = time.perf_counter()
        warm_time = (t1 - t0) * 1000.0
        results['warm_run_ms'].append(warm_time)

        cycle_end = time.perf_counter()
        if capture_cycle:
            cycle_times.append((cycle_end - cycle_start) * 1000.0)

        if sleep_between_ms is not None:
            remain = (sleep_between_ms / 1000.0) - (cycle_end - cycle_start)
            if remain > 0:
                time.sleep(remain)

        if idle_every and idle_duration_ms and (i + 1) % idle_every == 0:
            time.sleep(idle_duration_ms / 1000.0)

        if (i + 1) % 10 == 0:
            print(f"  å®Œæˆ {i+1}/{warm_repeats} æ¬¡")
    
    # ç»Ÿè®¡
    cold_avg = results['cold_start_ms'][0]
    warm_avg = np.mean(results['warm_run_ms'])
    warm_std = np.std(results['warm_run_ms'])
    
    print(f"\nğŸ“Š ç»Ÿè®¡ç»“æœ:")
    print(f"  å†·å¯åŠ¨: {cold_avg:.2f} ms (å«åŠ è½½ï¼Œn=1)")
    print(f"  çƒ­è¿è¡Œ: {warm_avg:.2f} Â± {warm_std:.2f} ms (n={warm_repeats})")
    print(f"  åŠ é€Ÿæ¯”: {cold_avg/warm_avg:.2f}x")
    
    results['statistics'] = {
        'cold_avg_ms': cold_avg,
        'cold_std_ms': 0.0,
        'warm_avg_ms': warm_avg,
        'warm_std_ms': warm_std,
        'speedup': cold_avg / warm_avg if warm_avg > 0 else 0,
    }
    
    if cycle_times is not None:
        results.setdefault('statistics', {})['cycle_avg_ms'] = float(np.mean(cycle_times)) if cycle_times else 0.0
        results['statistics']['cycle_std_ms'] = float(np.std(cycle_times)) if cycle_times else 0.0
        results['cycle_ms'] = cycle_times

    results['metadata'] = {
        'sleep_between_ms': sleep_between_ms,
        'idle_every': idle_every,
        'idle_duration_ms': idle_duration_ms,
        'warmup': warmup,
        'warm_repeats': warm_repeats,
    }

    return results


def main():
    import argparse
    parser = argparse.ArgumentParser(description='å››æ¨¡å‹å•ç‹¬æ¨ç†æµ‹è¯•')
    parser.add_argument('--cold-repeats', type=int, default=10, help='å†·å¯åŠ¨é‡å¤æ¬¡æ•°')
    parser.add_argument('--warm-repeats', type=int, default=50, help='çƒ­è¿è¡Œé‡å¤æ¬¡æ•°')
    parser.add_argument('--warmup', type=int, default=50, help='çƒ­è¿è¡Œå‰çš„é¢„çƒ­æ¬¡æ•°')
    parser.add_argument('--output', type=str, default='four_models_benchmark.json', help='è¾“å‡ºJSONæ–‡ä»¶')
    parser.add_argument('--models', type=str, nargs='+',
                        choices=['ssd_mobilenet_v2', 'ssd_mobilenet_v2_joint', 'efficientdet_lite2_448', 'deeplabv3_dm05', 'mobilenet_v2', 'mobilenet_v2_joint', 'netvlad_head', 'all'],
                        default=['all'], help='é€‰æ‹©è¦æµ‹è¯•çš„æ¨¡å‹')
    args = parser.parse_args()
    
    # ç¡®å®šè¦æµ‹è¯•çš„æ¨¡å‹
    if 'all' in args.models:
        # é»˜è®¤å››æ¨¡å‹ï¼šSSD MobileNet V2 + DeepLab + MobileNetV2(backbone) + NetVLAD head
        # æ³¨æ„ï¼šå¦‚æœä½ æƒ³è·‘è”åˆç¼–è¯‘ç‰ˆï¼ŒæŠŠä¸‹é¢ä¸¤ä¸ªé”®æ›¿æ¢æˆ *_joint å³å¯
        models_to_test = ['ssd_mobilenet_v2', 'deeplabv3_dm05', 'mobilenet_v2', 'netvlad_head']
    else:
        models_to_test = args.models
    
    print(f"\nğŸš€ å››æ¨¡å‹å•ç‹¬æ¨ç†æµ‹è¯•")
    print(f"{'='*60}")
    print(f"å†·å¯åŠ¨é‡å¤: {args.cold_repeats} æ¬¡")
    print(f"çƒ­è¿è¡Œé‡å¤: {args.warm_repeats} æ¬¡")
    print(f"é¢„çƒ­æ¬¡æ•°: {args.warmup} æ¬¡")
    print(f"æµ‹è¯•æ¨¡å‹: {', '.join(models_to_test)}")
    print(f"{'='*60}")
    
    all_results = {}
    
    # æµ‹è¯•æ¯ä¸ªæ¨¡å‹
    for model_key in models_to_test:
        model_path = MODELS[model_key]
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(model_path):
            print(f"\nâš ï¸  æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            continue
        
        try:
            if model_key == 'netvlad_head':
                results = test_cpu_netvlad(
                    model_path, 
                    model_key,
                    cold_repeats=args.cold_repeats,
                    warm_repeats=args.warm_repeats,
                    warmup=args.warmup
                )
            elif model_key == 'deeplabv3_dm05':
                # DeepLabv3 ä½¿ç”¨ TPU ç‰ˆæœ¬
                results = test_tpu_model(
                    model_path, 
                    model_key,
                    cold_repeats=args.cold_repeats,
                    warm_repeats=args.warm_repeats,
                    warmup=args.warmup,
                    use_tpu=True
                )
            else:
                # SSD å’Œ MobileNet ä½¿ç”¨ TPU
                results = test_tpu_model(
                    model_path, 
                    model_key,
                    cold_repeats=args.cold_repeats,
                    warm_repeats=args.warm_repeats,
                    warmup=args.warmup,
                    use_tpu=True
                )
            
            all_results[model_key] = results
            
        except Exception as e:
            print(f"\nâŒ æµ‹è¯•å¤±è´¥ ({model_key}): {e}")
            import traceback
            traceback.print_exc()
    
    # ä¿å­˜ç»“æœ
    output_path = os.path.join('/home/10210/Desktop/OS/results', args.output)
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    with open(output_path, 'w') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    # æ‰“å°æ±‡æ€»
    print(f"\n{'='*60}")
    print(f"ğŸ“Š æ±‡æ€»ç»Ÿè®¡")
    print(f"{'='*60}")
    for model_key, results in all_results.items():
        stats = results.get('statistics', {})
        print(f"\n{model_key}:")
        print(f"  å†·å¯åŠ¨: {stats.get('cold_avg_ms', 0):.2f} Â± {stats.get('cold_std_ms', 0):.2f} ms")
        print(f"  çƒ­è¿è¡Œ: {stats.get('warm_avg_ms', 0):.2f} Â± {stats.get('warm_std_ms', 0):.2f} ms")
        print(f"  åŠ é€Ÿæ¯”: {stats.get('speedup', 0):.2f}x")


if __name__ == '__main__':
    main()
